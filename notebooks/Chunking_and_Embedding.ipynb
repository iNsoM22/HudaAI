{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q supabase"
      ],
      "metadata": {
        "id": "QVHqBTMfjTG8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import numpy as np\n",
        "from contextvars import ContextVar\n",
        "from supabase import create_client\n",
        "from dotenv import load_dotenv\n",
        "import os"
      ],
      "metadata": {
        "id": "HEqvOa3Xk-s4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "\n",
        "SUPABASE_URL = userdata.get(\"SUPABASE_URL\")\n",
        "SUPABASE_KEY = userdata.get(\"SUPABASE_KEY\")\n",
        "\n",
        "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n"
      ],
      "metadata": {
        "id": "WquFRxZmjcsP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Check if the Supabase Conenction is Live"
      ],
      "metadata": {
        "id": "zU-P5ue1lByR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "supabase.table(\"surahs\").select(\"*\").limit(5).execute()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VieYViQPjtwe",
        "outputId": "3b0022d5-7d41-4895-df7d-3b5ae43b1fd5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "APIResponse(data=[{'id': 1, 'number': 1, 'name_simple': 'Al-Faatiha', 'name_arabic': 'سُورَةُ ٱلْفَاتِحَةِ', 'name_english': 'The Opening', 'revelation_place': 'Meccan', 'verses_count': 7}, {'id': 2, 'number': 2, 'name_simple': 'Al-Baqara', 'name_arabic': 'سُورَةُ البَقَرَةِ', 'name_english': 'The Cow', 'revelation_place': 'Medinan', 'verses_count': 286}, {'id': 3, 'number': 3, 'name_simple': 'Aal-i-Imraan', 'name_arabic': 'سُورَةُ آلِ عِمۡرَانَ', 'name_english': 'The Family of Imraan', 'revelation_place': 'Medinan', 'verses_count': 200}, {'id': 4, 'number': 4, 'name_simple': 'An-Nisaa', 'name_arabic': 'سُورَةُ النِّسَاءِ', 'name_english': 'The Women', 'revelation_place': 'Medinan', 'verses_count': 176}, {'id': 5, 'number': 5, 'name_simple': 'Al-Maaida', 'name_arabic': 'سُورَةُ المَائـِدَةِ', 'name_english': 'The Table', 'revelation_place': 'Medinan', 'verses_count': 120}], count=None)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verse Chunking\n",
        "\n",
        "To support Quran verse chunking, we will store the chunks in the database using the following table:\n",
        "\n",
        "### Table: `verse_chunks`\n",
        "\n",
        "| Column Name     | Data Type               | Constraints                  | Description |\n",
        "|-----------------|------------------------|-------------------------------|-------------|\n",
        "| `id`            | `bigserial`            | `NOT NULL`, Primary Key       | Unique identifier for each chunk |\n",
        "| `chunk_key`     | `character varying(50)`| `NOT NULL`                    | A unique key for the chunk (e.g., `2:1-5`) |\n",
        "| `surah_id`      | `integer`              | `NOT NULL`                    | ID of the Surah the chunk belongs to |\n",
        "| `start_verse`   | `integer`              | `NOT NULL`                    | Starting verse number of the chunk |\n",
        "| `end_verse`     | `integer`              | `NOT NULL`                    | Ending verse number of the chunk |\n",
        "| `text_uthmani`  | `text`                 | `NOT NULL`                    | Uthmani Arabic text of the chunk |\n",
        "| `text_simple`   | `text`                 | `NOT NULL`                    | Simplified Arabic text of the chunk |\n",
        "| `text_english`  | `text`                 | `NOT NULL`                    | English translation of the chunk |\n",
        "\n",
        "### Description\n",
        "\n",
        "- **Purpose:** Store chunks of Quran verses for easier retrieval and processing.  \n",
        "- **Chunking Strategy:** Each chunk may consist of a range of verses (`start_verse` → `end_verse`).  \n",
        "- **Unique Key:** `chunk_key` provides a quick reference for queries (e.g., `\"2:1-5\"`).  \n",
        "- **Text Fields:** All three editions of the Quran (Uthmani, Simple, English) are stored for multi-lingual access.\n"
      ],
      "metadata": {
        "id": "rPZeMqGslIra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_data(table_name,\n",
        "               fields=[],\n",
        "               filters={},\n",
        "               order_by=None,\n",
        "               desc=False,\n",
        "               offset=0,\n",
        "               limit=1000):\n",
        "\n",
        "  fields = fields or \"*\"\n",
        "  query = supabase.table(table_name).select(fields)\n",
        "\n",
        "  for key, value in filters.items():\n",
        "      query = query.eq(key, value)\n",
        "\n",
        "  if order_by:\n",
        "      query = query.order(order_by, desc=desc)\n",
        "\n",
        "  response = query.limit(limit).offset(offset).execute()\n",
        "\n",
        "  if not response or not response.data:\n",
        "    return []\n",
        "\n",
        "  return response.data\n"
      ],
      "metadata": {
        "id": "70_Wv8SSjsR5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_data(\"verses\", limit=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI0n6gRxnCKb",
        "outputId": "b0955bea-8851-4633-93f3-6acf3ebb3bc8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 1,\n",
              "  'surah_id': 1,\n",
              "  'verse_number': 1,\n",
              "  'verse_key': '1:1',\n",
              "  'juz_number': 1,\n",
              "  'ruku_number': 1,\n",
              "  'text_uthmani': '\\ufeffبِسْمِ ٱللَّهِ ٱلرَّحْمَٰنِ ٱلرَّحِيمِ',\n",
              "  'text_simple': '\\ufeffبِسْمِ اللَّهِ الرَّحْمَٰنِ الرَّحِيمِ',\n",
              "  'text_english': 'In the name of Allah, the Entirely Merciful, the Especially Merciful.'},\n",
              " {'id': 2,\n",
              "  'surah_id': 1,\n",
              "  'verse_number': 2,\n",
              "  'verse_key': '1:2',\n",
              "  'juz_number': 1,\n",
              "  'ruku_number': 1,\n",
              "  'text_uthmani': 'ٱلْحَمْدُ لِلَّهِ رَبِّ ٱلْعَٰلَمِينَ',\n",
              "  'text_simple': 'الْحَمْدُ لِلَّهِ رَبِّ الْعَالَمِينَ',\n",
              "  'text_english': '[All] praise is [due] to Allah, Lord of the worlds -'},\n",
              " {'id': 3,\n",
              "  'surah_id': 1,\n",
              "  'verse_number': 3,\n",
              "  'verse_key': '1:3',\n",
              "  'juz_number': 1,\n",
              "  'ruku_number': 1,\n",
              "  'text_uthmani': 'ٱلرَّحْمَٰنِ ٱلرَّحِيمِ',\n",
              "  'text_simple': 'الرَّحْمَٰنِ الرَّحِيمِ',\n",
              "  'text_english': 'The Entirely Merciful, the Especially Merciful,'},\n",
              " {'id': 4,\n",
              "  'surah_id': 1,\n",
              "  'verse_number': 4,\n",
              "  'verse_key': '1:4',\n",
              "  'juz_number': 1,\n",
              "  'ruku_number': 1,\n",
              "  'text_uthmani': 'مَٰلِكِ يَوْمِ ٱلدِّينِ',\n",
              "  'text_simple': 'مَالِكِ يَوْمِ الدِّينِ',\n",
              "  'text_english': 'Sovereign of the Day of Recompense.'},\n",
              " {'id': 5,\n",
              "  'surah_id': 1,\n",
              "  'verse_number': 5,\n",
              "  'verse_key': '1:5',\n",
              "  'juz_number': 1,\n",
              "  'ruku_number': 1,\n",
              "  'text_uthmani': 'إِيَّاكَ نَعْبُدُ وَإِيَّاكَ نَسْتَعِينُ',\n",
              "  'text_simple': 'إِيَّاكَ نَعْبُدُ وَإِيَّاكَ نَسْتَعِينُ',\n",
              "  'text_english': 'It is You we worship and You we ask for help.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "143227d7"
      },
      "source": [
        "def generate_verse_chunks(context_window=5):\n",
        "    \"\"\"\n",
        "    Fetches Quran verses, chunks them using a sliding window, and inserts them\n",
        "    into the `verse_chunks` table with separate fields for current verse text,\n",
        "    context window text, and current verse number.\n",
        "\n",
        "    Args:\n",
        "        context_window (int): Number of verses in each context window.\n",
        "    \"\"\"\n",
        "    print(f\"Starting verse chunking with context window: {context_window}\")\n",
        "\n",
        "    # Fetch all surahs\n",
        "    surahs_data = fetch_data(\"surahs\", limit=200)\n",
        "    if not surahs_data:\n",
        "        print(\"No surahs found. Aborting chunking process.\")\n",
        "        return\n",
        "\n",
        "    all_chunks_to_insert = []\n",
        "\n",
        "    for surah in surahs_data:\n",
        "        surah_id = surah['id']\n",
        "        surah_name = surah['name_simple']\n",
        "        surah_verses_count = surah['verses_count']\n",
        "\n",
        "        print(f\"Processing Surah: {surah_name} (ID: {surah_id}, Verses: {surah_verses_count})\")\n",
        "\n",
        "        # Fetch all verses for current surah\n",
        "        verses = fetch_data(\n",
        "            \"verses\",\n",
        "            filters={\"surah_id\": surah_id},\n",
        "            order_by=\"verse_number\",\n",
        "            desc=False,\n",
        "            limit=surah_verses_count\n",
        "        )\n",
        "\n",
        "        if not verses:\n",
        "            print(f\"No verses found for Surah {surah_name}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Clean BOM from text fields\n",
        "        for v in verses:\n",
        "            for field in ['text_uthmani', 'text_simple', 'text_english']:\n",
        "                if v.get(field):\n",
        "                    v[field] = v[field].lstrip('\\ufeff')\n",
        "\n",
        "        num_verses = len(verses)\n",
        "\n",
        "        # Generate chunks using sliding window\n",
        "        for i in range(num_verses):\n",
        "            current_verse = verses[i]\n",
        "\n",
        "            # Determine context window slice\n",
        "            context_start = i\n",
        "            context_end = min(i + context_window, num_verses)\n",
        "            context_verses = verses[context_start:context_end]\n",
        "\n",
        "            # Build context text\n",
        "            context_text_uthmani = \" \".join([v['text_uthmani'] for v in context_verses])\n",
        "            context_text_simple = \" \".join([v['text_simple'] for v in context_verses])\n",
        "            context_text_english = \" \".join([v['text_english'] for v in context_verses])\n",
        "\n",
        "            chunk_data = {\n",
        "                \"chunk_key\": f\"{surah_id}:{current_verse['verse_number']}\",\n",
        "                \"surah_id\": surah_id,\n",
        "                \"start_verse\": current_verse['verse_number'],\n",
        "                \"end_verse\": current_verse['verse_number'],\n",
        "                \"current_verse\": current_verse['verse_number'],\n",
        "                \"text_uthmani\": current_verse['text_uthmani'],\n",
        "                \"text_simple\": current_verse['text_simple'],\n",
        "                \"text_english\": current_verse['text_english'],\n",
        "                \"context_text_uthmani\": context_text_uthmani,\n",
        "                \"context_text_simple\": context_text_simple,\n",
        "                \"context_text_english\": context_text_english\n",
        "            }\n",
        "\n",
        "            all_chunks_to_insert.append(chunk_data)\n",
        "\n",
        "    batch_size = 500\n",
        "    total_batches = (len(all_chunks_to_insert) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(0, len(all_chunks_to_insert), batch_size):\n",
        "        batch = all_chunks_to_insert[i:i + batch_size]\n",
        "        try:\n",
        "            supabase.table(\"verse_chunks\").insert(batch).execute()\n",
        "            print(f\"  Inserted batch {i//batch_size + 1}/{total_batches} ({len(batch)} chunks).\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error inserting batch starting at index {i}. Details: {e}\")\n",
        "\n",
        "    print(\"Verse chunking process completed successfully.\")\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8fb5d9e9",
        "outputId": "e0ab63b2-b8ea-4808-a8b5-4bfc352b8059"
      },
      "source": [
        "generate_verse_chunks(context_window=5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting verse chunking with context window: 5\n",
            "Processing Surah: Al-Faatiha (ID: 1, Verses: 7)\n",
            "Processing Surah: Al-Baqara (ID: 2, Verses: 286)\n",
            "Processing Surah: Aal-i-Imraan (ID: 3, Verses: 200)\n",
            "Processing Surah: An-Nisaa (ID: 4, Verses: 176)\n",
            "Processing Surah: Al-Maaida (ID: 5, Verses: 120)\n",
            "Processing Surah: Al-An'aam (ID: 6, Verses: 165)\n",
            "Processing Surah: Al-A'raaf (ID: 7, Verses: 206)\n",
            "Processing Surah: Al-Anfaal (ID: 8, Verses: 75)\n",
            "Processing Surah: At-Tawba (ID: 9, Verses: 129)\n",
            "Processing Surah: Yunus (ID: 10, Verses: 109)\n",
            "Processing Surah: Hud (ID: 11, Verses: 123)\n",
            "Processing Surah: Yusuf (ID: 12, Verses: 111)\n",
            "Processing Surah: Ar-Ra'd (ID: 13, Verses: 43)\n",
            "Processing Surah: Ibrahim (ID: 14, Verses: 52)\n",
            "Processing Surah: Al-Hijr (ID: 15, Verses: 99)\n",
            "Processing Surah: An-Nahl (ID: 16, Verses: 128)\n",
            "Processing Surah: Al-Israa (ID: 17, Verses: 111)\n",
            "Processing Surah: Al-Kahf (ID: 18, Verses: 110)\n",
            "Processing Surah: Maryam (ID: 19, Verses: 98)\n",
            "Processing Surah: Taa-Haa (ID: 20, Verses: 135)\n",
            "Processing Surah: Al-Anbiyaa (ID: 21, Verses: 112)\n",
            "Processing Surah: Al-Hajj (ID: 22, Verses: 78)\n",
            "Processing Surah: Al-Muminoon (ID: 23, Verses: 118)\n",
            "Processing Surah: An-Noor (ID: 24, Verses: 64)\n",
            "Processing Surah: Al-Furqaan (ID: 25, Verses: 77)\n",
            "Processing Surah: Ash-Shu'araa (ID: 26, Verses: 227)\n",
            "Processing Surah: An-Naml (ID: 27, Verses: 93)\n",
            "Processing Surah: Al-Qasas (ID: 28, Verses: 88)\n",
            "Processing Surah: Al-Ankaboot (ID: 29, Verses: 69)\n",
            "Processing Surah: Ar-Room (ID: 30, Verses: 60)\n",
            "Processing Surah: Luqman (ID: 31, Verses: 34)\n",
            "Processing Surah: As-Sajda (ID: 32, Verses: 30)\n",
            "Processing Surah: Al-Ahzaab (ID: 33, Verses: 73)\n",
            "Processing Surah: Saba (ID: 34, Verses: 54)\n",
            "Processing Surah: Faatir (ID: 35, Verses: 45)\n",
            "Processing Surah: Yaseen (ID: 36, Verses: 83)\n",
            "Processing Surah: As-Saaffaat (ID: 37, Verses: 182)\n",
            "Processing Surah: Saad (ID: 38, Verses: 88)\n",
            "Processing Surah: Az-Zumar (ID: 39, Verses: 75)\n",
            "Processing Surah: Ghafir (ID: 40, Verses: 85)\n",
            "Processing Surah: Fussilat (ID: 41, Verses: 54)\n",
            "Processing Surah: Ash-Shura (ID: 42, Verses: 53)\n",
            "Processing Surah: Az-Zukhruf (ID: 43, Verses: 89)\n",
            "Processing Surah: Ad-Dukhaan (ID: 44, Verses: 59)\n",
            "Processing Surah: Al-Jaathiya (ID: 45, Verses: 37)\n",
            "Processing Surah: Al-Ahqaf (ID: 46, Verses: 35)\n",
            "Processing Surah: Muhammad (ID: 47, Verses: 38)\n",
            "Processing Surah: Al-Fath (ID: 48, Verses: 29)\n",
            "Processing Surah: Al-Hujuraat (ID: 49, Verses: 18)\n",
            "Processing Surah: Qaaf (ID: 50, Verses: 45)\n",
            "Processing Surah: Adh-Dhaariyat (ID: 51, Verses: 60)\n",
            "Processing Surah: At-Tur (ID: 52, Verses: 49)\n",
            "Processing Surah: An-Najm (ID: 53, Verses: 62)\n",
            "Processing Surah: Al-Qamar (ID: 54, Verses: 55)\n",
            "Processing Surah: Ar-Rahmaan (ID: 55, Verses: 78)\n",
            "Processing Surah: Al-Waaqia (ID: 56, Verses: 96)\n",
            "Processing Surah: Al-Hadid (ID: 57, Verses: 29)\n",
            "Processing Surah: Al-Mujaadila (ID: 58, Verses: 22)\n",
            "Processing Surah: Al-Hashr (ID: 59, Verses: 24)\n",
            "Processing Surah: Al-Mumtahana (ID: 60, Verses: 13)\n",
            "Processing Surah: As-Saff (ID: 61, Verses: 14)\n",
            "Processing Surah: Al-Jumu'a (ID: 62, Verses: 11)\n",
            "Processing Surah: Al-Munaafiqoon (ID: 63, Verses: 11)\n",
            "Processing Surah: At-Taghaabun (ID: 64, Verses: 18)\n",
            "Processing Surah: At-Talaaq (ID: 65, Verses: 12)\n",
            "Processing Surah: At-Tahrim (ID: 66, Verses: 12)\n",
            "Processing Surah: Al-Mulk (ID: 67, Verses: 30)\n",
            "Processing Surah: Al-Qalam (ID: 68, Verses: 52)\n",
            "Processing Surah: Al-Haaqqa (ID: 69, Verses: 52)\n",
            "Processing Surah: Al-Ma'aarij (ID: 70, Verses: 44)\n",
            "Processing Surah: Nooh (ID: 71, Verses: 28)\n",
            "Processing Surah: Al-Jinn (ID: 72, Verses: 28)\n",
            "Processing Surah: Al-Muzzammil (ID: 73, Verses: 20)\n",
            "Processing Surah: Al-Muddaththir (ID: 74, Verses: 56)\n",
            "Processing Surah: Al-Qiyaama (ID: 75, Verses: 40)\n",
            "Processing Surah: Al-Insaan (ID: 76, Verses: 31)\n",
            "Processing Surah: Al-Mursalaat (ID: 77, Verses: 50)\n",
            "Processing Surah: An-Naba (ID: 78, Verses: 40)\n",
            "Processing Surah: An-Naazi'aat (ID: 79, Verses: 46)\n",
            "Processing Surah: Abasa (ID: 80, Verses: 42)\n",
            "Processing Surah: At-Takwir (ID: 81, Verses: 29)\n",
            "Processing Surah: Al-Infitaar (ID: 82, Verses: 19)\n",
            "Processing Surah: Al-Mutaffifin (ID: 83, Verses: 36)\n",
            "Processing Surah: Al-Inshiqaaq (ID: 84, Verses: 25)\n",
            "Processing Surah: Al-Burooj (ID: 85, Verses: 22)\n",
            "Processing Surah: At-Taariq (ID: 86, Verses: 17)\n",
            "Processing Surah: Al-A'laa (ID: 87, Verses: 19)\n",
            "Processing Surah: Al-Ghaashiya (ID: 88, Verses: 26)\n",
            "Processing Surah: Al-Fajr (ID: 89, Verses: 30)\n",
            "Processing Surah: Al-Balad (ID: 90, Verses: 20)\n",
            "Processing Surah: Ash-Shams (ID: 91, Verses: 15)\n",
            "Processing Surah: Al-Lail (ID: 92, Verses: 21)\n",
            "Processing Surah: Ad-Dhuhaa (ID: 93, Verses: 11)\n",
            "Processing Surah: Ash-Sharh (ID: 94, Verses: 8)\n",
            "Processing Surah: At-Tin (ID: 95, Verses: 8)\n",
            "Processing Surah: Al-Alaq (ID: 96, Verses: 19)\n",
            "Processing Surah: Al-Qadr (ID: 97, Verses: 5)\n",
            "Processing Surah: Al-Bayyina (ID: 98, Verses: 8)\n",
            "Processing Surah: Az-Zalzala (ID: 99, Verses: 8)\n",
            "Processing Surah: Al-Aadiyaat (ID: 100, Verses: 11)\n",
            "Processing Surah: Al-Qaari'a (ID: 101, Verses: 11)\n",
            "Processing Surah: At-Takaathur (ID: 102, Verses: 8)\n",
            "Processing Surah: Al-Asr (ID: 103, Verses: 3)\n",
            "Processing Surah: Al-Humaza (ID: 104, Verses: 9)\n",
            "Processing Surah: Al-Fil (ID: 105, Verses: 5)\n",
            "Processing Surah: Quraish (ID: 106, Verses: 4)\n",
            "Processing Surah: Al-Maa'un (ID: 107, Verses: 7)\n",
            "Processing Surah: Al-Kawthar (ID: 108, Verses: 3)\n",
            "Processing Surah: Al-Kaafiroon (ID: 109, Verses: 6)\n",
            "Processing Surah: An-Nasr (ID: 110, Verses: 3)\n",
            "Processing Surah: Al-Masad (ID: 111, Verses: 5)\n",
            "Processing Surah: Al-Ikhlaas (ID: 112, Verses: 4)\n",
            "Processing Surah: Al-Falaq (ID: 113, Verses: 5)\n",
            "Processing Surah: An-Naas (ID: 114, Verses: 6)\n",
            "  Inserted batch 1/13 (500 chunks).\n",
            "  Inserted batch 2/13 (500 chunks).\n",
            "  Inserted batch 3/13 (500 chunks).\n",
            "  Inserted batch 4/13 (500 chunks).\n",
            "  Inserted batch 5/13 (500 chunks).\n",
            "  Inserted batch 6/13 (500 chunks).\n",
            "  Inserted batch 7/13 (500 chunks).\n",
            "  Inserted batch 8/13 (500 chunks).\n",
            "  Inserted batch 9/13 (500 chunks).\n",
            "  Inserted batch 10/13 (500 chunks).\n",
            "  Inserted batch 11/13 (500 chunks).\n",
            "  Inserted batch 12/13 (500 chunks).\n",
            "  Inserted batch 13/13 (236 chunks).\n",
            "Verse chunking process completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Different Embeddings\n",
        "\n",
        "As the chunking process is finished, we now move to **testing different embeddings**.\n",
        "\n",
        "### Embedding Table Schema\n",
        "\n",
        "We create the table `verse_embeddings` to store embeddings for each chunk:\n",
        "\n",
        "```sql\n",
        "CREATE TABLE public.verse_embeddings (\n",
        "    id BIGSERIAL NOT NULL,\n",
        "    chunk_id BIGINT NOT NULL,\n",
        "    embedding EXTENSIONS.VECTOR NOT NULL,\n",
        "    model_name VARCHAR(150) NOT NULL,\n",
        "    created_at TIMESTAMP WITHOUT TIME ZONE DEFAULT NOW()\n",
        ") TABLESPACE pg_default;\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "We will use the previously chunked data, generate embeddings for each chunk, and store them in this table according to the defined structure.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-iMy4U4mwIep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "Ihv_JurFvqtS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\" # 768 Dimensions\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35VHwbaC3m70",
        "outputId": "4b028bca-8a1c-4171-9294-4c8fcd006847"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MPNetModel(\n",
              "  (embeddings): MPNetEmbeddings(\n",
              "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): MPNetEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x MPNetLayer(\n",
              "        (attention): MPNetAttention(\n",
              "          (attn): MPNetSelfAttention(\n",
              "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (intermediate): MPNetIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): MPNetOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (relative_attention_bias): Embedding(32, 12)\n",
              "  )\n",
              "  (pooler): MPNetPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Converts a string into a fixed-size embedding using the transformer model.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        encoded_input = tokenizer(text,\n",
        "                                  padding=True,\n",
        "                                  truncation=True,\n",
        "                                  return_tensors=\"pt\").to(device)\n",
        "\n",
        "        model_output = model(**encoded_input)\n",
        "\n",
        "        # Mean pooling\n",
        "        token_embeddings = model_output.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
        "        attention_mask = encoded_input['attention_mask'].unsqueeze(-1)\n",
        "        masked_embeddings = token_embeddings * attention_mask\n",
        "        summed = masked_embeddings.sum(dim=1)\n",
        "        counts = attention_mask.sum(dim=1)\n",
        "        mean_pooled = summed / counts\n",
        "        return mean_pooled[0].cpu().numpy()"
      ],
      "metadata": {
        "id": "e_-ZXS4246Nr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_store_embeddings_batched(\n",
        "    table_name=\"verse_chunks\",\n",
        "    text_field=\"text_english\",\n",
        "    embedding_table=\"verse_embeddings\",\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "    batch_size=250,\n",
        "    fetch_limit=500\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates embeddings for verse chunks from a database table in batches\n",
        "    and inserts them into the embeddings table.\n",
        "\n",
        "    Args:\n",
        "        table_name (str): Table to fetch chunks from.\n",
        "        text_field (str): Text field to embed ('text_english', 'text_simple', 'text_uthmani').\n",
        "        embedding_table (str): Table to store embeddings.\n",
        "        model_name (str): HuggingFace transformer model.\n",
        "        batch_size (int): Number of embeddings to insert per batch.\n",
        "        fetch_limit (int): Number of rows to fetch per iteration from DB.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Loading embedding model: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    offset = 0\n",
        "    total_processed = 0\n",
        "\n",
        "    while True:\n",
        "        # Fetch batch of verse_chunks\n",
        "        chunks = fetch_data(table_name,\n",
        "                            offset=offset,\n",
        "                            limit=fetch_limit)\n",
        "        if not chunks:\n",
        "            print(\"No more rows to fetch. Finished processing all chunks.\")\n",
        "            break\n",
        "\n",
        "        embeddings_to_insert = []\n",
        "\n",
        "        for chunk in chunks:\n",
        "            chunk_id = chunk[\"id\"]\n",
        "            text_to_embed = chunk.get(text_field, \"\")\n",
        "            if not text_to_embed:\n",
        "                continue\n",
        "\n",
        "            # Remove BOM\n",
        "            text_to_embed = text_to_embed.lstrip('\\ufeff')\n",
        "            embedding_vector = get_embedding(text_to_embed)\n",
        "\n",
        "            embeddings_to_insert.append({\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"embedding\": embedding_vector.tolist(),\n",
        "                \"model_name\": model_name,\n",
        "                \"created_at\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "\n",
        "        for i in range(0, len(embeddings_to_insert), batch_size):\n",
        "            batch = embeddings_to_insert[i:i + batch_size]\n",
        "            try:\n",
        "                supabase.table(embedding_table).insert(batch).execute()\n",
        "                print(f\"Inserted batch of {len(batch)} embeddings (offset {offset})\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error inserting batch starting at offset {offset}: {e}\")\n",
        "\n",
        "        offset += fetch_limit\n",
        "        total_processed += len(chunks)\n",
        "        print(f\"Processed {total_processed} rows so far.\")\n",
        "\n",
        "    print(\"Embedding generation and insertion completed successfully.\")\n"
      ],
      "metadata": {
        "id": "_gjW1iMV5pYb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_and_store_embeddings_batched(\n",
        "    table_name=\"verse_chunks\",\n",
        "    text_field=\"text_english\",\n",
        "    embedding_table=\"verse_embeddings\",\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "    batch_size=250,\n",
        "    fetch_limit=500\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlU2ZClqFpDq",
        "outputId": "aa19873c-efe6-4e89-82ed-e8a06ebddba7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model: sentence-transformers/all-mpnet-base-v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1291749910.py:57: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"created_at\": datetime.utcnow().isoformat()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted batch of 250 embeddings (offset 0)\n",
            "Inserted batch of 250 embeddings (offset 0)\n",
            "Processed 500 rows so far.\n",
            "Inserted batch of 250 embeddings (offset 500)\n",
            "Inserted batch of 250 embeddings (offset 500)\n",
            "Processed 1000 rows so far.\n",
            "Inserted batch of 250 embeddings (offset 1000)\n",
            "Inserted batch of 250 embeddings (offset 1000)\n",
            "Processed 1500 rows so far.\n",
            "Inserted batch of 250 embeddings (offset 1500)\n",
            "Inserted batch of 250 embeddings (offset 1500)\n",
            "Processed 2000 rows so far.\n",
            "Inserted batch of 250 embeddings (offset 2000)\n",
            "Inserted batch of 250 embeddings (offset 2000)\n",
            "Processed 2500 rows so far.\n",
            "Inserted batch of 250 embeddings (offset 2500)\n",
            "Inserted batch of 250 embeddings (offset 2500)\n",
            "Processed 3000 rows so far.\n",
            "Inserted batch of 250 embeddings (offset 3000)\n",
            "Inserted batch of 250 embeddings (offset 3000)\n",
            "Processed 3500 rows so far.\n",
            "Inserted batch of 250 embeddings (offset 3500)\n",
            "Inserted batch of 250 embeddings (offset 3500)\n",
            "Processed 4000 rows so far.\n",
            "Inserted batch of 250 embeddings (offset 4000)\n",
            "Inserted batch of 250 embeddings (offset 4000)\n",
            "Processed 4500 rows so far.\n",
            "Inserted batch of 250 embeddings (offset 4500)\n",
            "Inserted batch of 250 embeddings (offset 4500)\n",
            "Processed 5000 rows so far.\n",
            "Inserted batch of 250 embeddings (offset 5000)\n",
            "Inserted batch of 250 embeddings (offset 5000)\n",
            "Processed 5500 rows so far.\n",
            "Inserted batch of 250 embeddings (offset 5500)\n",
            "Inserted batch of 250 embeddings (offset 5500)\n",
            "Processed 6000 rows so far.\n",
            "Inserted batch of 236 embeddings (offset 6000)\n",
            "Processed 6236 rows so far.\n",
            "No more rows to fetch. Finished processing all chunks.\n",
            "Embedding generation and insertion completed successfully.\n"
          ]
        }
      ]
    }
  ]
}